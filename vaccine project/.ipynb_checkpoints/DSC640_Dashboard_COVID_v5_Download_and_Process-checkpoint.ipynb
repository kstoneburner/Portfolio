{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** GEOPANDAS sources\n",
    "#https://jcutrer.com/python/learn-geopandas-plotting-usmaps\n",
    "#https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html\n",
    "#https://towardsdatascience.com/lets-make-a-map-using-geopandas-pandas-and-matplotlib-to-make-a-chloropleth-map-dddc31c1983d\n",
    "#https://geopandas.org/docs/user_guide/mapping.html\n",
    "\n",
    "#//*** Build Custom Color Gradients\n",
    "#https://coolors.co/gradient-palette/ffffff-e0472b?number=9\n",
    "\n",
    "#//*** pandas_bokeh\n",
    "#//*** https://pythonawesome.com/bokeh-plotting-backend-for-pandas-and-geopandas/\n",
    "\n",
    "#//*** Clean geopandas install\n",
    "#//***conda create -n geopandas -c conda-forge python=3.8 geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hospitalization research:\n",
    "Data.gov search for tag: hhs-covid-19\n",
    "\n",
    "posible lead: https://healthdata.gov/dataset/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/4cnb-m4rz\n",
    "\n",
    "possible lead individual facility breakdown by week May - October 2021: https://healthdata.gov/dataset/COVID-19-Hospital-Data-Coverage-Report-v4wn-auj8-A/ewep-8fwa\n",
    "\n",
    "This look like State Level Data: https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/g62h-syeh\n",
    "\n",
    "Data CSV Link: https://healthdata.gov/api/views/g62h-syeh/rows.csv?accessType=DOWNLOAD\n",
    "\n",
    "Healthdata.gov Hospitalization Search: https://healthdata.gov/browse?q=hospitalization&sortBy=relevance\n",
    "\n",
    "Hospital Facility TimeSeries: https://healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/anag-cw7u\n",
    "\n",
    "\n",
    "Other Sources:\n",
    " - COVID-19 State and County Policy Orders (infographic?): https://catalog.data.gov/dataset/covid-19-state-and-county-policy-orders-9408a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a> \n",
    "## Internal Links ##\n",
    "\n",
    "- Working [working](#working).\n",
    "\n",
    "- [Load/Rebuild County Level Combined Data Set: Vaccination, Cases, Deaths: rebuild county_daily_df](#rebuild_county_daily_df)\n",
    "\n",
    "- [Load/Rebuild State Level Combined Data Set: Vaccination, Cases, Deaths: rebuild state_daily_df](#rebuild_state_daily_df)\n",
    "\n",
    "- [ Build State Level: state_case_death_df - cases & deaths only ](#build_state_case_death_df)\n",
    "\n",
    "- [ Load or Rebuild County Level Vaccine Data Set: county_vax_df](#rebuild_county_vax_df)\n",
    "\n",
    "- [ Build Vaccine Tiers: County Data: county_tier_df](#build_tiers_county)\n",
    "\n",
    "- [Build Vaccine Tiers: Statewide Data](#build_tiers_statewide)\n",
    "\n",
    "- [ Build: hospital_df ](#build_hospital_df)\n",
    "\n",
    "- [ Build: state_hosp_tiers_df](#state_hosp_tiers_df)\n",
    "\n",
    "## Analyze ##\n",
    "- [ Analyze State Hospitalization Tiers: state_hosp_tiers_df](#analyze_state_hosp_tiers_df)\n",
    "\n",
    "## Graphs ##\n",
    "- [ Blog Graphs ](#build_blog_graphs)\n",
    "\n",
    "## HTML Processing ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDC Community Transmission Guidelines\n",
    "\n",
    "https://covid.cdc.gov/covid-data-tracker/#county-view\n",
    "\n",
    "\n",
    "\n",
    "![](transmission_risk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"working\"></a> \n",
    "# Working #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "  \n",
       "\n",
       "  const inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.0.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# //*** Imports and Load Data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "#//*** Use the whole window in the IPYNB editor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import time \n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import bokeh\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import DatetimeTickFormatter\n",
    "from bokeh.embed import components\n",
    "from bokeh.models import ColumnDataSource, Grid, HBar, LinearAxis, Plot\n",
    "\n",
    "import pandas_bokeh\n",
    "pandas_bokeh.output_notebook()\n",
    "\n",
    "#https://docs.bokeh.org/en/latest/docs/reference/models/formatters.html\n",
    "#https://docs.bokeh.org/en/latest/docs/user_guide/embed.html\n",
    "\n",
    "#//*** Bokeh Categorical Labels\n",
    "#https://docs.bokeh.org/en/0.12.10/docs/user_guide/categorical.html\n",
    "\n",
    "#//*** Color Palettes: https://docs.bokeh.org/en/latest/docs/reference/palettes.html\n",
    "#//*** Hover Tools: https://docs.bokeh.org/en/latest/docs/user_guide/tools.html\n",
    "from datetime import datetime\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib\n",
    "\n",
    "#//*** Maximize columns and rows displayed by pandas\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "#//*** Holds \n",
    "out_plot = {}\n",
    "out_stats = {}\n",
    "out_images = []\n",
    "out_exec_summary_images = []\n",
    "\n",
    "med_figsize_x =634\n",
    "med_figsize_y = 512\n",
    "\n",
    "#df_list = []\n",
    "\n",
    "#from sklearn import linear_model\n",
    "#from math import sqrt\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\"\n",
    "\n",
    "#Hospitalizations - State\n",
    "#https://www.cdc.gov/nhsn/covid19/report-patient-impact.html#anchor_1594393649\n",
    "#DL: https://www.cdc.gov/nhsn/pdfs/covid19/covid19-NatEst.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Control #\n",
    " * use_cached_files = True - Use the Locally cached versions of the data\n",
    " * use_cached_files = False - Download updated source Data\n",
    " \n",
    " * rebuild_master_files = True - Rebuild county_daily_df. Takes 5-10 minutes\n",
    " * rebuild_master_files = False - Uses the cached version of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//************************************************************************\n",
    "#//*** If True, will rely on locally stored files\n",
    "#//*** If False, will Download current data and rebuild all data sets\n",
    "#//*** Should only be set to False if Data needs to be updated\n",
    "#//************************************************************************\n",
    "use_cached_files = False\n",
    "#rebuild_master_files = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filepaths #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//*** Raw Filenames\n",
    "confirmed_data_filename = \"z_us_confirmed.csv\"\n",
    "death_data_filename = \"z_us_death_cases.csv\"\n",
    "vaccine_data_filename = \"z_us_vaccination.csv\"\n",
    "county_vaccine_data_filename = \"z_us_county_vaccination.csv.zip\"\n",
    "state_hospital_filename = \"z_state_hospital.csv\"\n",
    "\n",
    "#//*** Filenames for Caching files locally\n",
    "attrib_confirm_filename = \"attrib_confirm_df.dat.zip\"\n",
    "attrib_death_filename =  \"attrib_death_df.dat.zip\"\n",
    "all_dates_filename = \"all_dates.json\"\n",
    "geo_vax_filename = \"geo_vax_df.dat\"\n",
    "casevax_filename = \"casevax.dat.zip\"\n",
    "county_daily_df_filename = \"z_county_daily_df.csv.zip\"\n",
    "state_daily_df_filename = \"z_state_daily_df.csv.zip\"\n",
    "hospital_df_filename = \"z_hospital_df.csv.zip\"\n",
    "\n",
    "county_tier_df_filename = \"z_county_tier_df.csv.zip\"\n",
    "whole_county_tier_df_filename = \"z_whole_county_tier_df.csv.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Confirmed Data Downloaded\n",
      "US Confirmed Data Written to file.\n",
      "US Deaths Data Downloaded\n",
      "US Death Data Written to file.\n",
      "Vaccination Data Downloading\n",
      "US Vaccination Data Written to file.\n",
      "County Vaccination Data Downloading\n",
      "US County Vaccination Data Written to file.\n",
      "Hospitalization Data Downloading\n",
      "US Hospitalization Data Written to file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#CDC Data: https://catalog.data.gov/dataset/covid-19-vaccinations-in-the-united-statescounty\n",
    "\n",
    "#//***********************************************************************************************\n",
    "#//*** California COVID Data website:\n",
    "#//**************************************\n",
    "#//*** https://data.chhs.ca.gov/dataset/covid-19-time-series-metrics-by-county-and-state\n",
    "#//***********************************************************************************************\n",
    "\n",
    "#//*** Always Download and Process files for this script\n",
    "if True:\n",
    "    try:\n",
    "        response = requests.get(\"https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\")\n",
    "        if response.ok:\n",
    "            print(\"US Confirmed Data Downloaded\")\n",
    "            f = open(confirmed_data_filename, \"w\")\n",
    "            f.write(response.text)\n",
    "            f.close()\n",
    "            print(\"US Confirmed Data Written to file.\")\n",
    "    except:\n",
    "        print(\"US Confirmed Data: Trouble Downloading From Johns Hopkins Github\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(\"https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\")\n",
    "        if response.ok:\n",
    "            print(\"US Deaths Data Downloaded\")\n",
    "            f = open(death_data_filename, \"w\")\n",
    "            f.write(response.text)\n",
    "            f.close()\n",
    "            print(\"US Death Data Written to file.\")\n",
    "    except:\n",
    "        print(\"US Death Data: Trouble Downloading From Johns Hopkins Github\")\n",
    "        \n",
    "    try:\n",
    "        #response = requests.get(\"https://data.cdc.gov/api/views/8xkx-amqh/rows.csv?accessType=DOWNLOAD\")\n",
    "        response = requests.get(\"https://data.cdc.gov/api/views/unsk-b7fc/rows.csv?accessType=DOWNLOAD\")\n",
    "        if response.ok:\n",
    "            print(\"Vaccination Data Downloading\")\n",
    "            f = open(vaccine_data_filename, \"w\")\n",
    "            f.write(response.text)\n",
    "            f.close()\n",
    "            print(\"US Vaccination Data Written to file.\")\n",
    "    except:\n",
    "        print(\"US Vaccine Data: Trouble Downloading From CDC\")\n",
    "\n",
    "    try:\n",
    "        #//*** CDC Vaccination County Data\n",
    "        #//*** Source: https://data.cdc.gov/Vaccinations/COVID-19-Vaccinations-in-the-United-States-County/8xkx-amqh\n",
    "        response = requests.get(\"https://data.cdc.gov/api/views/8xkx-amqh/rows.csv?accessType=DOWNLOAD\")\n",
    "        if response.ok:\n",
    "            print(\"County Vaccination Data Downloading\")\n",
    "            #//*** Write CSV File\n",
    "            f = open(county_vaccine_data_filename.replace(\".zip\",\"\"), \"w\")\n",
    "            f.write(response.text)\n",
    "            f.close()\n",
    "\n",
    "            #//*** File > 100 mb\n",
    "            #//*** read the CSV into a Dataframe and pickle the file with compression\n",
    "            pd.read_csv(county_vaccine_data_filename.replace(\".zip\",\"\")).to_pickle(county_vaccine_data_filename)\n",
    "\n",
    "            #//*** Delete the Original CSV File\n",
    "            os.remove(county_vaccine_data_filename.replace(\".zip\",\"\"))\n",
    "\n",
    "            print(\"US County Vaccination Data Written to file.\")\n",
    "    except:\n",
    "        print(\"US County Vaccine Data: Trouble Downloading From CDC\")    \n",
    "\n",
    "    #Hospitalizations - State\n",
    "    #https://healthdata.gov/api/views/g62h-syeh/rows.csv?accessType=DOWNLOAD\n",
    "\n",
    "    try:\n",
    "        #response = requests.get(\"https://data.cdc.gov/api/views/8xkx-amqh/rows.csv?accessType=DOWNLOAD\")\n",
    "        response = requests.get(\"https://healthdata.gov/api/views/g62h-syeh/rows.csv?accessType=DOWNLOAD\")\n",
    "        if response.ok:\n",
    "            print(\"Hospitalization Data Downloading\")\n",
    "            f = open(state_hospital_filename, \"w\")\n",
    "            f.write(response.text)\n",
    "            f.close()\n",
    "            print(\"US Hospitalization Data Written to file.\")\n",
    "    except:\n",
    "        print(\"US Hospitalization: Trouble Downloading From Healthdata.gov\")\n",
    "\n",
    "#confirm_df = pd.read_csv(confirmed_data_filename, dtype={\"FIPS\":np.int32})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rebuild_county_daily_df\"></a> \n",
    "# Load or Rebuild County Level Combined Data Set: Cases & Deaths #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding county_daily_df...This will take a while.\n",
      "Loading Raw Confirm Cases Data....\n",
      "Loading Raw Deaths Data....\n",
      "Begin Merge Confirm and Deaths Columns with Vaccination Rows....\n",
      "Working: 100 of 3195\n",
      "Working: 200 of 3195\n",
      "Working: 300 of 3195\n",
      "Working: 400 of 3195\n",
      "Working: 500 of 3195\n",
      "Working: 600 of 3195\n",
      "Working: 700 of 3195\n",
      "Working: 800 of 3195\n",
      "Working: 900 of 3195\n",
      "Working: 1000 of 3195\n",
      "Working: 1100 of 3195\n",
      "Working: 1200 of 3195\n",
      "Working: 1300 of 3195\n",
      "Working: 1400 of 3195\n",
      "Working: 1500 of 3195\n",
      "Working: 1600 of 3195\n",
      "Working: 1700 of 3195\n",
      "Working: 1800 of 3195\n",
      "Working: 1900 of 3195\n",
      "Working: 2000 of 3195\n",
      "Working: 2100 of 3195\n",
      "Working: 2200 of 3195\n",
      "Working: 2300 of 3195\n",
      "Working: 2400 of 3195\n",
      "Working: 2500 of 3195\n",
      "Working: 2600 of 3195\n",
      "Working: 2700 of 3195\n",
      "Working: 2800 of 3195\n",
      "Working: 2900 of 3195\n",
      "Working: 3000 of 3195\n",
      "Working: 3100 of 3195\n",
      "Writing county daily to File: z_county_daily_df.csv.zip\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#use_cached_files=False3\n",
    "#if use_cached_files:\n",
    "    #print(\"Using Cached county_daily_df...\")\n",
    "    #county_daily_df = pd.read_pickle(county_daily_df_filename)\n",
    "if True:\n",
    "    print(\"Rebuilding county_daily_df...This will take a while.\")\n",
    "    \"\"\"\n",
    "    #//***************************************\n",
    "    #//*** Build Vaccine Geographic Data\n",
    "    #//***************************************\n",
    "    print(\"Loading County Geopandas Shape File\")\n",
    "    #//*** Get Shape File\n",
    "    county_df = gpd.read_file('maps\\cb_2018_us_county_20m.shp')\n",
    "\n",
    "\n",
    "    #//*** Keep the important Columns\n",
    "    county_df = county_df[['STATEFP','GEOID','NAME','geometry']]\n",
    "\n",
    "    county_df['GEOID'] = county_df['GEOID'].astype(int)\n",
    "    county_df['STATEFP'] = county_df['STATEFP'].astype(int)\n",
    "    \n",
    "    #print(counties['GEOID'].unique(),county_df['FIPS'].unique())\n",
    "    \n",
    "    \n",
    "    #print(\"Merge Shape Data with Vaccine Data...\")\n",
    "    #county_df = counties.merge(county_df,left_on=\"GEOID\",right_on=\"FIPS\")\n",
    "\n",
    "    #//*** Round the Vaccination Percentages\n",
    "    #county_df[\"Series_Complete_Pop_Pct\"] = county_df[\"Series_Complete_Pop_Pct\"].round(1)\n",
    "    #county_df[\"Administered_Dose1_Pop_Pct\"] = county_df[\"Administered_Dose1_Pop_Pct\"].round(1)\n",
    "\n",
    "    #county_df['Date'] = county_df['Date'].apply(lambda x: datetime.strptime(x, \"%m/%d/%Y\").date())\n",
    "\n",
    "\n",
    "    \n",
    "    #print(\"Dropping Duplicates...\")\n",
    "    #county_df = county_df.drop_duplicates()\n",
    "    \"\"\"\n",
    "    \n",
    "    #//****************************************************\n",
    "    #//*** Prepare Confirmed Cases and Deaths For Merge\n",
    "    #//****************************************************\n",
    "\n",
    "    print(\"Loading Raw Confirm Cases Data....\")\n",
    "    confirm_df = pd.read_csv(confirmed_data_filename)\n",
    "\n",
    "    confirm_df = confirm_df[confirm_df['Admin2'] != 'Unassigned']\n",
    "\n",
    "    #//*** Convert Confirmed Date Columns to Date Objects\n",
    "    cols = []\n",
    "    confirm_date_cols = []\n",
    "    for col in confirm_df.columns:\n",
    "        if \"/\" not in col:\n",
    "            cols.append(col)\n",
    "        else:\n",
    "            cols.append(datetime.strptime(col, \"%m/%d/%y\").date())\n",
    "            confirm_date_cols.append(datetime.strptime(col, \"%m/%d/%y\").date())\n",
    "\n",
    "    confirm_df.columns = cols\n",
    "\n",
    "    print(\"Loading Raw Deaths Data....\")\n",
    "\n",
    "    death_df = pd.read_csv(death_data_filename)\n",
    "\n",
    "    death_df\n",
    "\n",
    "    death_df['Province_State'].unique()\n",
    "    death_df = death_df[death_df['iso2'] =='US']\n",
    "    death_df = death_df[death_df['Province_State'] != \"Diamond Princess\"]\n",
    "    death_df = death_df[death_df['Province_State'] != \"Grand Princess\"]\n",
    "    death_df = death_df[death_df['Admin2'] != 'Unassigned']\n",
    "    death_df.dropna(inplace=True)\n",
    "    death_df['FIPS'] = death_df['FIPS'].astype(int)\n",
    "\n",
    "\n",
    "    #//*** Convert Confirmed Date Columns to Date Objects\n",
    "    cols = []\n",
    "    death_date_cols = []\n",
    "\n",
    "    for col in death_df.columns:\n",
    "        if \"/\" not in col:\n",
    "            cols.append(col)\n",
    "        else:\n",
    "            cols.append(datetime.strptime(col, \"%m/%d/%y\").date())\n",
    "            death_date_cols.append(datetime.strptime(col, \"%m/%d/%y\").date())\n",
    "\n",
    "    death_df.columns = cols\n",
    "\n",
    "\n",
    "    ##///**** REBUILD COUNTY_DAILY_DF - This takes a while 15ish Minutes\n",
    "\n",
    "\n",
    "    #//*** Integrate Confirmed and Deaths with Vaccine Data. Build derived Values\n",
    "    i = 0\n",
    "\n",
    "    print(\"Begin Merge Confirm and Deaths Columns with Vaccination Rows....\")\n",
    "    county_daily_df = pd.DataFrame()\n",
    "\n",
    "    #//*** Get Min and Max Values\n",
    "    #start_date = county_df['Date'].min()\n",
    "    #end_date = county_df['Date'].max()\n",
    "\n",
    "    #if np.array(confirm_date_cols).max() < end_date:\n",
    "    #    end_date = np.array(confirm_date_cols).max()\n",
    "\n",
    "    #print(start_date,end_date)\n",
    "\n",
    "    #print(county_df)\n",
    "    #//*** Loop Through Each FIPS County\n",
    "    for FIPS in death_df.sort_values(['FIPS'])['FIPS'].unique():\n",
    "\n",
    "    \n",
    "        i += 1\n",
    "\n",
    "        attrib = death_df[death_df['FIPS'] == FIPS]\n",
    "            \n",
    "            \n",
    "\n",
    "        #loop_df = pd.concat([loop_df] * (len(death_df[death_df['FIPS']==GEOID])),ignore_index=True)\n",
    "        \n",
    "        #//*** Merge Combined Key and Population. Grab a subset of FIPS from death_df\n",
    "        #loop_df = loop_df.merge(death_df[death_df['FIPS']==GEOID][['FIPS','Combined_Key','Population']],left_on='GEOID',right_on='FIPS')\n",
    "        \n",
    "        #//*** Get Confirmed Values for FIPS County\n",
    "        loop_df = confirm_df[confirm_df['FIPS']==FIPS][confirm_date_cols].transpose()\n",
    "        \n",
    "        loop_df = loop_df.reset_index()\n",
    "        \n",
    "        loop_df.columns = ['Date','tot_confirm']\n",
    "        \n",
    "\n",
    "\n",
    "        #//*** Build Total Deaths for FIPS County\n",
    "        ds = death_df[death_df['FIPS']==FIPS][death_date_cols].transpose()\n",
    "\n",
    "        ds = ds.reset_index()\n",
    "        ds.columns = ['Date','tot_deaths']\n",
    "        del ds['Date']\n",
    "        \n",
    "        #//*** Keep Relevant Columns\n",
    "                \n",
    "        for col in ['FIPS','Admin2','Province_State','Combined_Key','Population']:\n",
    "            loop_df[col]=attrib[col].iloc[0]\n",
    "\n",
    "        loop_df = loop_df[['Date','FIPS','Admin2','Province_State','Combined_Key','Population','tot_confirm']]\n",
    "\n",
    "        #//*** Generate new rows based on length of death series\n",
    "        #loop_df = pd.concat([loop_df] * len(ds),ignore_index=True)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #//*** Join Confirmed Values\n",
    "        #loop_df = loop_df.join(cs)\n",
    "        \n",
    "        #loop_df = cs\n",
    "        \n",
    "        #//*** Merge Death Values\n",
    "        loop_df = loop_df.join(ds)\n",
    "\n",
    "        #//*** Build New Confirmed Cases\n",
    "        loop_df['New_Confirm']  = loop_df['tot_confirm'].diff()\n",
    "        #//*** Reset Negative Cases to 0\n",
    "        loop_df.loc[loop_df['New_Confirm'] < 0,f'New_Confirm']=0\n",
    "        \n",
    "\n",
    "        #//*** Build New Death Cases\n",
    "        loop_df['New_Deaths']  = loop_df['tot_deaths'].diff()\n",
    "        \n",
    "        #//*** Reset Negative Deaths to 0\n",
    "        loop_df.loc[loop_df['New_Deaths'] < 0,f'New_Deaths']=0\n",
    "        #print(cs)\n",
    "        #print(ds)\n",
    "\n",
    "        #//*** Build New Confirmed 7 Day Average\n",
    "        loop_df['case_7_day_avg']  = loop_df['New_Confirm'].rolling(7).mean()\n",
    "\n",
    "        #//*** Build New Deaths 7 Day Average\n",
    "        loop_df['death_7_day_avg']  = loop_df['New_Deaths'].rolling(7).mean()\n",
    "\n",
    "        #//*** Build New Confirmed 100k 7 day  Average\n",
    "        loop_df['case_100k_avg']  = loop_df['case_7_day_avg'] / (loop_df['Population'] / 100000 )\n",
    "\n",
    "        #//*** Build New Confirmed 100k 7 day  Average\n",
    "        loop_df['death_100k_avg']  = loop_df['death_7_day_avg'] / (loop_df['Population'] / 100000 )\n",
    "        \n",
    "        #//*** Set scaled Values to a max of 100 for heatmap purposes\n",
    "        loop_df['case_scaled_100k'] = loop_df['case_100k_avg']\n",
    "        loop_df['death_scaled_100k'] = loop_df['death_100k_avg']\n",
    "        \n",
    "        loop_df.loc[loop_df[f\"case_scaled_100k\"] > 100,f\"case_scaled_100k\"]=100\n",
    "        loop_df.loc[loop_df[f\"death_scaled_100k\"] > 5,f\"death_scaled_100k\"]=5\n",
    "        \n",
    "        \n",
    "        #loop_df['Date'] = loop_df['Date'].apply(lambda x: datetime.strptime(x, \"%m/%d/%Y\").date())\n",
    "        #print(vax_df[vax_df['FIPS']==GEOID])\n",
    "\n",
    "        #loop_df = loop_df[ loop_df['Date'] >= vax_df['Date'].min() ]\n",
    "        #del loop_df['FIPS']\n",
    "        #loop_df = loop_df.merge(vax_df[vax_df['FIPS'] == GEOID],left_on='Date',right_on='Date',how='left')\n",
    "\n",
    "\n",
    "        #//*** All Data merged and Calculated. Merge with temporary Dataframe()\n",
    "        county_daily_df = pd.concat([county_daily_df,loop_df])\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Working: {i} of {len(death_df['FIPS'].unique())}\")\n",
    "\n",
    "    county_daily_df = county_daily_df.dropna()\n",
    "    print(f\"Writing county daily to File: {county_daily_df_filename}\")\n",
    "    county_daily_df.to_pickle(county_daily_df_filename)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rebuild_state_daily_df\"></a> \n",
    "# Load or Rebuild State Level Combined Data Set: Vaccination, Cases, Deaths #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding state_daily_df...This goes quickly.\n",
      "       state population      Date tot_death\n",
      "0    Alabama    4903185   1/22/20         0\n",
      "1    Alabama    4903185   1/23/20         0\n",
      "2    Alabama    4903185   1/24/20         0\n",
      "3    Alabama    4903185   1/25/20         0\n",
      "4    Alabama    4903185   1/26/20         0\n",
      "..       ...        ...       ...       ...\n",
      "662  Wyoming     578759  11/14/21      1298\n",
      "663  Wyoming     578759  11/15/21      1298\n",
      "664  Wyoming     578759  11/16/21      1347\n",
      "665  Wyoming     578759  11/17/21      1347\n",
      "666  Wyoming     578759  11/18/21      1347\n",
      "\n",
      "[38686 rows x 4 columns]\n",
      "     STATEFP  GEOID        NAME                                           geometry        Name Postal Code  FIPS       Date Location  Administered_Dose1_Recip  Administered_Dose1_Pop_Pct  \\\n",
      "0          6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2021-11-18       CA                  30499532                        77.2   \n",
      "1          6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2021-11-17       CA                  30410779                        77.0   \n",
      "2          6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2021-11-16       CA                  30361433                        76.8   \n",
      "3          6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2021-11-15       CA                  30330016                        76.8   \n",
      "4          6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2021-11-14       CA                  30172467                        76.4   \n",
      "..       ...    ...         ...                                                ...         ...         ...   ...        ...      ...                       ...                         ...   \n",
      "335        6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2020-12-18       CA                         0                         0.0   \n",
      "336        6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2020-12-17       CA                         0                         0.0   \n",
      "337        6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2020-12-16       CA                         0                         0.0   \n",
      "338        6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2020-12-15       CA                         0                         0.0   \n",
      "339        6      6  California  MULTIPOLYGON (((-118.59397 33.46720, -118.4847...  California          CA     6 2020-12-14       CA                         0                         0.0   \n",
      "\n",
      "     Administered_Dose1_Recip_12Plus  Administered_Dose1_Recip_12PlusPop_Pct  Administered_Dose1_Recip_18Plus  Administered_Dose1_Recip_18PlusPop_Pct  Administered_Dose1_Recip_65Plus  \\\n",
      "0                           30091556                                    89.4                         27810647                                    90.8                          6232609   \n",
      "1                           30031839                                    89.3                         27756481                                    90.7                          6217822   \n",
      "2                           30002082                                    89.2                         27730545                                    90.6                          6211405   \n",
      "3                           29985004                                    89.1                         27715372                                    90.5                          6208337   \n",
      "4                           29902512                                    88.9                         27643049                                    90.3                          6190159   \n",
      "..                               ...                                     ...                              ...                                     ...                              ...   \n",
      "335                                0                                     0.0                                0                                     0.0                                0   \n",
      "336                                0                                     0.0                                0                                     0.0                                0   \n",
      "337                                0                                     0.0                                0                                     0.0                                0   \n",
      "338                                0                                     0.0                                0                                     0.0                                0   \n",
      "339                                0                                     0.0                                0                                     0.0                                0   \n",
      "\n",
      "     Administered_Dose1_Recip_65PlusPop_Pct  Series_Complete_Yes  Series_Complete_Pop_Pct  Series_Complete_12Plus  Series_Complete_12PlusPop_Pct  Series_Complete_18Plus  \\\n",
      "0                                      99.9             24692255                     62.5                24642593                           73.2                22781839   \n",
      "1                                      99.9             24656243                     62.4                24607033                           73.1                22749196   \n",
      "2                                      99.9             24639061                     62.4                24589865                           73.1                22733744   \n",
      "3                                      99.9             24629034                     62.3                24579843                           73.1                22724775   \n",
      "4                                      99.9             24574846                     62.2                24525696                           72.9                22676252   \n",
      "..                                      ...                  ...                      ...                     ...                            ...                     ...   \n",
      "335                                     0.0                    0                      0.0                       0                            0.0                       0   \n",
      "336                                     0.0                    0                      0.0                       0                            0.0                       0   \n",
      "337                                     0.0                    0                      0.0                       0                            0.0                       0   \n",
      "338                                     0.0                    0                      0.0                       0                            0.0                       0   \n",
      "339                                     0.0                    0                      0.0                       0                            0.0                       0   \n",
      "\n",
      "     Series_Complete_18PlusPop_Pct  Series_Complete_65Plus  Series_Complete_65PlusPop_Pct       state population tot_death tot_confirm New_Cases  case_7_day_avg case_avg_100k New_Deaths  \\\n",
      "0                             74.4                 4938175                           84.6  California   39512223     73552     5028939      9952    10345.571429     25.187143        253   \n",
      "1                             74.3                 4931037                           84.5  California   39512223     73405     5023913      9993     9906.714286     25.290908        172   \n",
      "2                             74.3                 4927720                           84.4  California   39512223     73299     5018987      9259     9807.000000     23.433255        188   \n",
      "3                             74.2                 4926295                           84.4  California   39512223     73233     5013920     10433    10164.000000     26.404488        128   \n",
      "4                             74.1                 4915885                           84.2  California   39512223     73111     5009728      8964    11477.714286     22.686651         97   \n",
      "..                             ...                     ...                            ...         ...        ...       ...         ...       ...             ...           ...        ...   \n",
      "335                            0.0                       0                            0.0  California   39512223     22322     1877459     90251    83991.857143    228.412864        533   \n",
      "336                            0.0                       0                            0.0  California   39512223     22057     1831732    100643    82027.428571    254.713586        667   \n",
      "337                            0.0                       0                            0.0  California   39512223     21789     1787208    111785    77725.428571    282.912455        635   \n",
      "338                            0.0                       0                            0.0  California   39512223     21390     1731089     89046    71221.857143    225.363174        327   \n",
      "339                            0.0                       0                            0.0  California   39512223     21154     1675423     63990    67147.571429    161.949886        183   \n",
      "\n",
      "     death_7_day_avg death_avg_100k  \n",
      "0         162.857143       0.640308  \n",
      "1         166.571429       0.435308  \n",
      "2         172.428571       0.475802  \n",
      "3         167.857143        0.32395  \n",
      "4         168.571429       0.245494  \n",
      "..               ...            ...  \n",
      "335       426.000000        1.34895  \n",
      "336       400.428571       1.688085  \n",
      "337       357.428571       1.607098  \n",
      "338       319.857143       0.827592  \n",
      "339       306.142857       0.463148  \n",
      "\n",
      "[340 rows x 35 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic Vaccinations Built\n"
     ]
    }
   ],
   "source": [
    "#//******************************\n",
    "#//**** Build State Vaccine Data\n",
    "#//**** Orginally geo_vax_df...Now: state_daily_df\n",
    "#//******************************\n",
    "\n",
    "#if use_cached_files:\n",
    "#    print(\"Using Cached state_daily_df...\")\n",
    "#    state_daily_df = pd.read_pickle(state_daily_df_filename)\n",
    "if True:\n",
    "    print(\"Rebuilding state_daily_df...This goes quickly.\")\n",
    "\n",
    "    #//*** read Raw Vaccine csv\n",
    "    vax_df = pd.read_csv(vaccine_data_filename)\n",
    "    #//**** Load Confirmed Data\n",
    "    confirm_df = pd.read_csv(confirmed_data_filename)\n",
    "    confirm_df = confirm_df.dropna(subset=[\"FIPS\"])\n",
    "\n",
    "    #//**** Load death Data\n",
    "    death_df = pd.read_csv(death_data_filename)\n",
    "    death_df = death_df[death_df[\"Admin2\"] != \"Unassigned\"]\n",
    "    death_df = death_df.dropna(subset=[\"FIPS\"])\n",
    "\n",
    "\n",
    "    #//*** Filter Columns to get just the Completed Values\n",
    "    cols = ['Date','Location']\n",
    "    filter_val = ['FedLTC','Unk','Janssen','Pfizer','Moderna','Fed_LTC']\n",
    "    for col in vax_df.columns:\n",
    "        if 'Complete' in col or 'Dose1' in col:\n",
    "            skipVal = False\n",
    "            for val in filter_val:\n",
    "                if val in col:\n",
    "                    skipVal = True\n",
    "                    continue\n",
    "            if skipVal:\n",
    "                continue\n",
    "            cols.append(col)\n",
    "\n",
    "    #//*** Keep Relevant Data\n",
    "    vax_df = vax_df[cols]\n",
    "\n",
    "    #//***************************************\n",
    "    #//*** Build Vaccine Geographic Data\n",
    "    #//***************************************\n",
    "\n",
    "    #//*** Get Shape File\n",
    "    states = gpd.read_file('maps\\cb_2018_us_state_20m.shp')\n",
    "\n",
    "    #//*** Keep the important Columns\n",
    "    states = states[['STATEFP','GEOID','NAME','geometry']]\n",
    "\n",
    "    states['GEOID'] = states['GEOID'].astype(int)\n",
    "    states['STATEFP'] = states['STATEFP'].astype(int)\n",
    "\n",
    "    #//*** Vaccine Info only has State Abbreviations. Need to add FIPS codes to merge with geographic data\n",
    "    #//*** Load DF with States and FIPS values\n",
    "    state_fips = pd.read_csv(\"state_fips.csv\")\n",
    "    geo_vax_df = state_fips.merge(vax_df,left_on=\"Postal Code\",right_on=\"Location\")\n",
    "\n",
    "    #//*** remove States not in continental US\n",
    "    geo_vax_df = geo_vax_df[geo_vax_df[\"Postal Code\"] != \"AK\" ]\n",
    "    geo_vax_df = geo_vax_df[geo_vax_df[\"Postal Code\"] != \"HI\" ]\n",
    "    geo_vax_df = geo_vax_df[geo_vax_df[\"Postal Code\"] != \"AS\" ]\n",
    "    geo_vax_df = geo_vax_df[geo_vax_df[\"Postal Code\"] != \"GU\" ]\n",
    "    geo_vax_df = geo_vax_df[geo_vax_df[\"Postal Code\"] != \"MP\" ]\n",
    "    geo_vax_df = geo_vax_df[geo_vax_df[\"Postal Code\"] != \"PR\" ]\n",
    "    geo_vax_df = geo_vax_df[geo_vax_df[\"Postal Code\"] != \"VI\" ]\n",
    "\n",
    "    #//*** Merge Geographic Data with Vaccince Data\n",
    "    geo_vax_df = states.merge(geo_vax_df,left_on=\"STATEFP\",right_on=\"FIPS\")\n",
    "\n",
    "    #//*** Round the Vaccination Percentages\n",
    "    geo_vax_df[\"Series_Complete_Pop_Pct\"] = geo_vax_df[\"Series_Complete_Pop_Pct\"].round(1)\n",
    "    geo_vax_df[\"Administered_Dose1_Pop_Pct\"] = geo_vax_df[\"Administered_Dose1_Pop_Pct\"].round(1)\n",
    "\n",
    "    geo_vax_df['Date'] = geo_vax_df['Date'].apply(lambda x: datetime.strptime(x, \"%m/%d/%Y\"))\n",
    "\n",
    "\n",
    "    #//*** Extract Deaths and Confirmed Cases From JOhns Hopkins Column wise data and merge with Vaccine Row Data\n",
    "\n",
    "    #//***Process Deaths\n",
    "    us_deaths_df = pd.DataFrame()\n",
    "\n",
    "    for group in death_df.groupby('Province_State'):\n",
    "        loop_stats_df = group[1][group[1].columns[12:]]\n",
    "        #print(loop_stats_df.columns)\n",
    "\n",
    "        base_row = pd.Series([group[0],group[1]['Population'].sum()], index=['state','population'])\n",
    "\n",
    "        base_row = pd.DataFrame(base_row).transpose()\n",
    "        stats_df = pd.DataFrame([[group[0]],loop_stats_df.sum().index,loop_stats_df.sum()]).transpose()\n",
    "        stats_df.columns = ['state','Date','tot_death']\n",
    "        stats_df['state'] = stats_df['state'].fillna(group[0])\n",
    "        base_row = base_row.merge(stats_df,left_on=\"state\",right_on=\"state\")\n",
    "        us_deaths_df = pd.concat([us_deaths_df,base_row])\n",
    "\n",
    "    print(us_deaths_df)\n",
    "\n",
    "    us_df = pd.DataFrame()\n",
    "    #//***Process confirmed and merge with deaths\n",
    "    for group in confirm_df.groupby('Province_State'):\n",
    "        loop_stats_df = group[1][group[1].columns[11:]]\n",
    "        #print(loop_stats_df.columns)\n",
    "        stats_df = pd.DataFrame([loop_stats_df.sum().index,loop_stats_df.sum()]).transpose()\n",
    "        stats_df.columns = ['Date','tot_confirm']\n",
    "\n",
    "        us_df = pd.concat([us_df,us_deaths_df[us_deaths_df['state']==group[0]].merge(stats_df,left_on='Date',right_on='Date')])\n",
    "\n",
    "\n",
    "    us_df['Date'] = us_df['Date'].apply(lambda x: datetime.strptime(x, \"%m/%d/%y\"))    \n",
    "\n",
    "\n",
    "    #//*** Build additional stat values and merge with geo_vax_df\n",
    "    #//*** t_vax_df is a temp dataframe\n",
    "    t_vax_df = pd.DataFrame()\n",
    "    for group in us_df.groupby('state'):\n",
    "        loop_df = group[1].copy()\n",
    "        #print(loop_df)\n",
    "        loop_df['New_Cases'] = loop_df['tot_confirm'].copy().diff(2)\n",
    "\n",
    "        #//*** Replace Values that Are less than zero with Zero.\n",
    "        loop_df.loc[loop_df['New_Cases'] < 0,f'New_Cases']=0\n",
    "\n",
    "        loop_df['case_7_day_avg'] = loop_df['New_Cases'].rolling(7).mean()\n",
    "        loop_df['case_avg_100k'] = loop_df['New_Cases'] / (loop_df['population'] / 100000)\n",
    "        loop_df['New_Deaths'] = loop_df['tot_death'].copy().diff(2)\n",
    "\n",
    "                #//*** Replace Values that Are less than zero with Zero.\n",
    "        loop_df.loc[loop_df['New_Deaths'] < 0,f'New_Deaths']=0\n",
    "        loop_df['death_7_day_avg'] = loop_df['New_Deaths'].rolling(7).mean()\n",
    "        loop_df['death_avg_100k'] = loop_df['New_Deaths'] / (loop_df['population'] / 100000)\n",
    "        t_vax_df = pd.concat([t_vax_df,geo_vax_df[geo_vax_df['Name']==group[0]].merge(loop_df,left_on=\"Date\", right_on='Date')])\n",
    "\n",
    "\n",
    "    print(t_vax_df[t_vax_df['state']=='California'])\n",
    "    geo_vax_df = t_vax_df.sort_values(by=['NAME',\"Date\"]).copy()\n",
    "\n",
    "    #//*** Cleanup Column Names\n",
    "    ren_cols = {\n",
    "    'Administered_Dose1_Recip' : 'first_dose_count',\n",
    "    'Administered_Dose1_Pop_Pct' : 'first_dose_pct',\n",
    "    'Series_Complete_Yes' : 'total_vaccinated_count',\n",
    "    'Series_Complete_Pop_Pct' : 'total_vaccinated_percent',\n",
    "    }\n",
    "    #//*** Columns to remove\n",
    "    del_cols = [ \n",
    "        'Administered_Dose1_Recip_12Plus',\n",
    "         'Administered_Dose1_Recip_12PlusPop_Pct',\n",
    "         'Administered_Dose1_Recip_18Plus',\n",
    "         'Administered_Dose1_Recip_18PlusPop_Pct',\n",
    "         'Administered_Dose1_Recip_65Plus',\n",
    "         'Administered_Dose1_Recip_65PlusPop_Pct',\n",
    "         'Series_Complete_12Plus',\n",
    "         'Series_Complete_12PlusPop_Pct',\n",
    "         'Series_Complete_18Plus',\n",
    "         'Series_Complete_18PlusPop_Pct',\n",
    "         'Series_Complete_65Plus',\n",
    "         'Series_Complete_65PlusPop_Pct',]\n",
    "    vax_cols = list(geo_vax_df.columns)\n",
    "\n",
    "    for find,replace in ren_cols.items():\n",
    "        vax_cols = [replace if i==find else i for i in vax_cols]\n",
    "\n",
    "    geo_vax_df.columns=vax_cols\n",
    "\n",
    "    for col in del_cols:\n",
    "        if col in geo_vax_df.columns:\n",
    "            del geo_vax_df[col]\n",
    "\n",
    "    state_daily_df = geo_vax_df\n",
    "\n",
    "\n",
    "    #//*** Write vaccinations to file\n",
    "    state_daily_df.to_pickle(state_daily_df_filename)        \n",
    "    print(\"Geographic Vaccinations Built\")\n",
    "\n",
    "    #//*** Temp File Cleanup\n",
    "    del geo_vax_df\n",
    "    del us_deaths_df\n",
    "    del us_df\n",
    "    del t_vax_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rebuild_column_data_for_bokeh\"></a> \n",
    "# Load or Rebuild Columwise Dataframes for Bokeh Slider Plots: attrib_confirm_df, attrib_death_df #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     STATEFP  GEOID      NAME                                           geometry  Province_State                Combined_Key\n",
      "0         37  37017    Bladen  POLYGON ((-78.90200 34.83527, -78.79960 34.850...  North Carolina  Bladen, North Carolina, US\n",
      "1         37  37167    Stanly  POLYGON ((-80.49737 35.20210, -80.29542 35.502...  North Carolina  Stanly, North Carolina, US\n",
      "2         39  39153    Summit  POLYGON ((-81.68699 41.13596, -81.68495 41.277...            Ohio            Summit, Ohio, US\n",
      "3         42  42113  Sullivan  POLYGON ((-76.81373 41.59003, -76.22014 41.541...    Pennsylvania  Sullivan, Pennsylvania, US\n",
      "4         48  48459    Upshur  POLYGON ((-95.15274 32.66095, -95.15211 32.902...           Texas           Upshur, Texas, US\n",
      "...      ...    ...       ...                                                ...             ...                         ...\n",
      "3137      22  22003     Allen  POLYGON ((-93.13029 30.59789, -92.97917 30.598...       Louisiana        Allen, Louisiana, US\n",
      "3138      38  38005    Benson  POLYGON ((-99.84661 48.37130, -99.49292 48.370...    North Dakota    Benson, North Dakota, US\n",
      "3139      31  31159    Seward  POLYGON ((-97.36812 41.04695, -96.91094 41.046...        Nebraska        Seward, Nebraska, US\n",
      "3140      37  37023     Burke  POLYGON ((-81.90665 35.88338, -81.94319 35.960...  North Carolina   Burke, North Carolina, US\n",
      "3141      13  13261    Sumter  POLYGON ((-84.43301 32.04196, -84.43121 32.134...         Georgia         Sumter, Georgia, US\n",
      "\n",
      "[3108 rows x 6 columns]\n",
      "Begin Building Case Attributes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\family\\anaconda3\\envs\\Cartopy\\lib\\site-packages\\geopandas\\geodataframe.py:1351: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#if use_cached_files:\n",
    "    #//*** Load Locally stored data\n",
    "    #attrib_confirm_df = pd.read_pickle(attrib_confirm_filename)\n",
    "    #print(\"Loading Cached: confirm_df\")\n",
    "    #attrib_death_df = pd.read_pickle(attrib_death_filename)\n",
    "    #print(\"Loading Cached: death_df\")\n",
    "\n",
    "    #print(\"Load Geo Vaccine Data\")\n",
    "    #geo_vax_df = pd.read_pickle(geo_vax_filename)\n",
    "\n",
    "    #print(\"Load Casevax_df: Combined Case, Death, Vaccination Data \")\n",
    "    #casevax_df = pd.read_pickle(casevax_filename)\n",
    "\n",
    "    #//**** Load death Data\n",
    "    #death_df = pd.read_csv(death_data_filename)\n",
    "    #confirm_df = pd.read_csv(confirmed_data_filename)   \n",
    "    #confirm_df = confirm_df.dropna(subset=[\"FIPS\"])\n",
    "\n",
    "    # Read all dates JSON\n",
    "    #with open(all_dates_filename, \"r\") as infile:\n",
    "    #    all_dates = json.loads(infile.read())['all_dates']\n",
    "    #print(\"Loading Cached: all_dates\")\n",
    "if True:\n",
    "    #    print(\"Re-Building Data...\")\n",
    "    #//**** Load Confirmed Data\n",
    "    confirm_df = pd.read_csv(confirmed_data_filename)\n",
    "    confirm_df = confirm_df.dropna(subset=[\"FIPS\"])\n",
    "\n",
    "    #//**** Load death Data\n",
    "    death_df = pd.read_csv(death_data_filename)\n",
    "    death_df = death_df[death_df[\"Admin2\"] != \"Unassigned\"]\n",
    "    death_df = death_df.dropna(subset=[\"FIPS\"])\n",
    "\n",
    "    #//*** Rename Columns to sync dataframes by FIPS/GEOID\n",
    "\n",
    "    ren_cols = {\n",
    "        \"FIPS\" : \"GEOID\"\n",
    "    }\n",
    "\n",
    "    #//*********************************\n",
    "    #//*** Rename Confirmed Cols\n",
    "    #//*********************************\n",
    "\n",
    "    #//*** Get columns\n",
    "    cols = np.array(confirm_df.columns)\n",
    "\n",
    "    #//*** Rplace Values\n",
    "    for find_col,replace_col in ren_cols.items():\n",
    "        cols = np.where(cols == find_col,replace_col,cols)\n",
    "\n",
    "    #//*** update columns\n",
    "    confirm_df.columns = cols\n",
    "\n",
    "    #//*********************************\n",
    "    #//*** Rename Death Cols\n",
    "    #//*********************************\n",
    "    #//*** Get columns\n",
    "    cols = np.array(death_df.columns)\n",
    "\n",
    "    #//*** Replace Values\n",
    "    for find_col,replace_col in ren_cols.items():\n",
    "        cols = np.where(cols == find_col,replace_col,cols)\n",
    "\n",
    "    #//*** update columns\n",
    "    death_df.columns = cols\n",
    "\n",
    "\n",
    "    confirm_df[\"GEOID\"] = confirm_df[\"GEOID\"].astype(int)\n",
    "    death_df[\"GEOID\"] =   death_df[\"GEOID\"].astype(int)\n",
    "\n",
    "    #//*** US States Only. Lose the Territories\n",
    "    confirm_df = confirm_df[confirm_df['iso3'] == 'USA']\n",
    "    death_df = death_df[death_df['iso3'] == 'USA']\n",
    "\n",
    "\n",
    "    #//*** Get Shape File\n",
    "    states = gpd.read_file('maps\\cb_2018_us_county_20m.shp')\n",
    "\n",
    "    #//*** Keep the important Columns\n",
    "    states = states[['STATEFP','GEOID','NAME','geometry']]\n",
    "\n",
    "\n",
    "    states['GEOID'] = states['GEOID'].astype(int)\n",
    "\n",
    "    #//*** Merge with Shapes\n",
    "    confirm_df = states.merge(confirm_df,left_on=\"GEOID\",right_on=\"GEOID\")\n",
    "    death_df = states.merge(death_df,left_on=\"GEOID\",right_on=\"GEOID\")\n",
    "\n",
    "    #//*** Remove Extra Columns\n",
    "    rem_cols = ['iso2','iso3','code3',\"Admin2\",\"Country_Region\",\"Lat\",\"Long_\",\"UID\"]\n",
    "\n",
    "    cols = list(confirm_df.columns)\n",
    "\n",
    "    for col in rem_cols:\n",
    "        cols.remove(col)\n",
    "\n",
    "    confirm_df = confirm_df[cols]\n",
    "\n",
    "    cols = list(death_df.columns)\n",
    "\n",
    "    for col in rem_cols:\n",
    "        cols.remove(col)\n",
    "\n",
    "    death_df = death_df[cols]\n",
    "\n",
    "    #//*** Convert Cumulative Cases to Daily Cases\n",
    "    confirm_df[confirm_df.columns[6:]] = confirm_df[confirm_df.columns[6:]].diff(axis = 1, periods = 1)\n",
    "    death_df[death_df.columns[7:]]     = death_df[death_df.columns[7:]].diff(axis = 1, periods = 1)\n",
    "\n",
    "    #//*** Convert to 7-day Rolling Mean of New Cases\n",
    "    confirm_df[confirm_df.columns[6:]] = confirm_df[confirm_df.columns[6:]].rolling(7,axis=1).mean()\n",
    "    death_df[death_df.columns[7:]] = death_df[death_df.columns[7:]].rolling(7,axis=1).mean()\n",
    "\n",
    "\n",
    "    #//*** Remove the single na column\n",
    "    confirm_df = confirm_df.dropna(axis=1)\n",
    "    death_df = death_df.dropna(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #//**** Merge Population with confirm. Pop_df is first to keep population stats away from the data columns that are added sequentially every day\n",
    "    confirm_df = confirm_df.merge(death_df[['GEOID','Population']].copy(),left_on=\"GEOID\",right_on=\"GEOID\")\n",
    "\n",
    "    #//*** Generate population 100k values\n",
    "    confirm_df['pop_100k'] = confirm_df['Population'] / 100000\n",
    "    death_df['pop_100k'] = death_df['Population'] / 100000\n",
    "\n",
    "    #//*** Filter out Hawaii and Alaska\n",
    "    confirm_df = confirm_df[confirm_df[\"Province_State\"] != \"Hawaii\"]\n",
    "    confirm_df = confirm_df[confirm_df[\"Province_State\"] != \"Alaska\"]\n",
    "\n",
    "    death_df = death_df[death_df[\"Province_State\"] != \"Hawaii\"]\n",
    "    death_df = death_df[death_df[\"Province_State\"] != \"Alaska\"]\n",
    "\n",
    "        #//*** Split Attribute Columns from Data Columns\n",
    "    attrib_confirm_df = confirm_df[confirm_df.columns[:6]].copy()\n",
    "    print(attrib_confirm_df)\n",
    "    data_confirm_df = confirm_df[confirm_df.columns[6:]].copy()\n",
    "\n",
    "    #//*** Move Last two Data Cols to attrib_confirm_df\n",
    "    last_cols = data_confirm_df.columns[-2:]\n",
    "\n",
    "    for col in last_cols:\n",
    "        #//*** Add Column to attrib_confirm_df\n",
    "        attrib_confirm_df[col] = data_confirm_df[col]\n",
    "\n",
    "        #//*** Delete Column from data_confirm_df\n",
    "        del data_confirm_df[col]\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "    #//*** Split Death Columns into attrib_death_df and data_death_df\n",
    "    attrib_death_df = death_df[death_df.columns[:7]].copy()\n",
    "    \n",
    "    data_death_df = death_df[death_df.columns[7:]].copy()\n",
    "\n",
    "    #//*** Move Last two Data Cols to attrib_confirm_df\n",
    "    last_cols = death_df.columns[-1:]\n",
    "\n",
    "    for col in last_cols:\n",
    "        #//*** Add Column to attrib_confirm_df\n",
    "        attrib_death_df[col] = data_death_df[col]\n",
    "\n",
    "        #//*** Delete Column from data_confirm_df\n",
    "        del data_death_df[col]\n",
    "\n",
    "\n",
    "\n",
    "    #//*** Rename Attrib Columns for Cleanliness and Vanity\n",
    "    ren_cols = {\n",
    "        \"POPESTIMATE\" : \"Population\",\n",
    "        \"Combined_Key\" : \"Loc\",\n",
    "        \"STNAME\" : \"State\",\n",
    "        \"CTYNAME\" : \"County\",\n",
    "        #\"9/17/21\" : \"New_Cases_9/17/21\"\n",
    "    }\n",
    "\n",
    "    #//*** Get columns\n",
    "    cols = np.array(attrib_confirm_df.columns)\n",
    "\n",
    "    #//*** Replace Values\n",
    "    for find_col,replace_col in ren_cols.items():\n",
    "        cols = np.where(cols == find_col,replace_col,cols)\n",
    "\n",
    "    #//*** Change Column Names    \n",
    "    attrib_confirm_df.columns = cols\n",
    "\n",
    "\n",
    "    #//**** clean Combined County Names\n",
    "    attrib_confirm_df['Loc'] = attrib_confirm_df['Loc'].str.replace(\", US\",\"\")\n",
    "\n",
    "    #//**** Rename DEATH_DF Columns\n",
    "    #//*** Get columns\n",
    "    cols = np.array(attrib_death_df.columns)\n",
    "\n",
    "    #//*** Replace Values\n",
    "    for find_col,replace_col in ren_cols.items():\n",
    "        cols = np.where(cols == find_col,replace_col,cols)\n",
    "\n",
    "    #//*** Change Column Names    \n",
    "    attrib_death_df.columns = cols\n",
    "\n",
    "\n",
    "    #//**** clean Combined County Names\n",
    "    attrib_confirm_df['Loc'] = attrib_confirm_df['Loc'].str.replace(\", US\",\"\")\n",
    "    attrib_death_df['Loc'] = attrib_death_df['Loc'].str.replace(\", US\",\"\")\n",
    "\n",
    "\n",
    "    #//*** Clean Attributes\n",
    "    attrib_confirm_df\n",
    "\n",
    "    #//**********************\n",
    "    #//*** Build Daily Data\n",
    "    #//**********************\n",
    "\n",
    "    #//*** Get All Dates\n",
    "    all_dates = list(data_confirm_df.columns)\n",
    "    all_dates\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"Begin Building Case Attributes...\")\n",
    "\n",
    "\n",
    "    #//*** Loop through each date and build daily derived Data\n",
    "    for date in all_dates:\n",
    "\n",
    "        #//*** Add New Cases for each Date\n",
    "        col = f\"{date}_New_Cases\"\n",
    "        attrib_confirm_df[col] = data_confirm_df[date].astype(int)\n",
    "        \n",
    "        col = f\"{date}_New_Deaths\"\n",
    "        attrib_death_df[col]   = data_death_df[date].astype(int)\n",
    "\n",
    "        #//*** Calculate New Cases Per 100k\n",
    "        col = f\"{date}_New_Cases_per_100k\"\n",
    "        attrib_confirm_df[col] = (data_confirm_df[date] / attrib_confirm_df['pop_100k']).astype(int)\n",
    "        \n",
    "        death_col = f\"{date}_New_Deaths_per_100k\"\n",
    "        attrib_death_df[death_col] =   (data_death_df[date] / attrib_death_df['pop_100k']).astype(int)\n",
    "\n",
    "        #//*** Build Scaled 100k cases (min=0, max=100)\n",
    "        #//*** Make a copy to date_scaled_100k\n",
    "        attrib_confirm_df[f\"{date}_scaled_100k\"] = attrib_confirm_df[col]\n",
    "        attrib_death_df[f\"{date}_scaled_100k\"]   = attrib_death_df[death_col]\n",
    "\n",
    "\n",
    "        #//*** Replace Values < 0 with 0. Negative values are due to adjustments from previous day values\n",
    "        attrib_confirm_df.loc[attrib_confirm_df[f\"{date}_scaled_100k\"] < 0,f\"{date}_scaled_100k\"]=0\n",
    "        attrib_death_df.loc[attrib_death_df[f\"{date}_scaled_100k\"] < 0,f\"{date}_scaled_100k\"]=0\n",
    "\n",
    "        #//*** Replace Values < 0 with 0. Negative values are due to adjustments from previous day values\n",
    "        #//*** Set Maximum community transmission to 100. This is the Max value on the CDC Scale, and it quiets this\n",
    "        attrib_confirm_df.loc[attrib_confirm_df[f\"{date}_scaled_100k\"] > 100,f\"{date}_scaled_100k\"]=100\n",
    "        attrib_death_df.loc[attrib_death_df[f\"{date}_scaled_100k\"] > 100,f\"{date}_scaled_100k\"]=100\n",
    "\n",
    "    #//*** Everything to a file\n",
    "    attrib_confirm_df.to_pickle(attrib_confirm_filename)\n",
    "    attrib_death_df.to_pickle(attrib_death_filename)\n",
    "\n",
    "\n",
    "    # Writing to sample.json\n",
    "    with open(all_dates_filename, \"w\") as outfile:\n",
    "        outfile.write(json.dumps({\"all_dates\" : all_dates}))\n",
    "\n",
    "\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rebuild_county_vax_df\"></a> \n",
    "# Load or Rebuild County Level Vaccine Data Set: county_vax_df #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Recip_County</th>\n",
       "      <th>Recip_State</th>\n",
       "      <th>total_vaccinated_percent</th>\n",
       "      <th>total_vaccinated_count</th>\n",
       "      <th>first_dose_pct</th>\n",
       "      <th>first_dose_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>5075</td>\n",
       "      <td>Lawrence County</td>\n",
       "      <td>AR</td>\n",
       "      <td>42.3</td>\n",
       "      <td>6938</td>\n",
       "      <td>49.7</td>\n",
       "      <td>8148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>1053</td>\n",
       "      <td>Escambia County</td>\n",
       "      <td>AL</td>\n",
       "      <td>33.1</td>\n",
       "      <td>12112</td>\n",
       "      <td>42.5</td>\n",
       "      <td>15552.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>19171</td>\n",
       "      <td>Tama County</td>\n",
       "      <td>IA</td>\n",
       "      <td>58.0</td>\n",
       "      <td>9777</td>\n",
       "      <td>61.9</td>\n",
       "      <td>10430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>6043</td>\n",
       "      <td>Mariposa County</td>\n",
       "      <td>CA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>19097</td>\n",
       "      <td>Jackson County</td>\n",
       "      <td>IA</td>\n",
       "      <td>49.2</td>\n",
       "      <td>9568</td>\n",
       "      <td>52.0</td>\n",
       "      <td>10100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122472</th>\n",
       "      <td>2020-12-13</td>\n",
       "      <td>42109</td>\n",
       "      <td>Snyder County</td>\n",
       "      <td>PA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122473</th>\n",
       "      <td>2020-12-13</td>\n",
       "      <td>17117</td>\n",
       "      <td>Macoupin County</td>\n",
       "      <td>IL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122474</th>\n",
       "      <td>2020-12-13</td>\n",
       "      <td>29189</td>\n",
       "      <td>St. Louis County</td>\n",
       "      <td>MO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122475</th>\n",
       "      <td>2020-12-13</td>\n",
       "      <td>36053</td>\n",
       "      <td>Madison County</td>\n",
       "      <td>NY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122476</th>\n",
       "      <td>2020-12-13</td>\n",
       "      <td>21017</td>\n",
       "      <td>Bourbon County</td>\n",
       "      <td>KY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1063143 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date   FIPS      Recip_County Recip_State  total_vaccinated_percent  total_vaccinated_count  first_dose_pct  first_dose_count\n",
       "0        2021-11-18   5075   Lawrence County          AR                      42.3                    6938            49.7            8148.0\n",
       "1        2021-11-18   1053   Escambia County          AL                      33.1                   12112            42.5           15552.0\n",
       "2        2021-11-18  19171       Tama County          IA                      58.0                    9777            61.9           10430.0\n",
       "3        2021-11-18   6043   Mariposa County          CA                       0.0                       0             0.0               0.0\n",
       "4        2021-11-18  19097    Jackson County          IA                      49.2                    9568            52.0           10100.0\n",
       "...             ...    ...               ...         ...                       ...                     ...             ...               ...\n",
       "1122472  2020-12-13  42109     Snyder County          PA                       0.0                       0             0.0               0.0\n",
       "1122473  2020-12-13  17117   Macoupin County          IL                       0.0                       0             0.0               0.0\n",
       "1122474  2020-12-13  29189  St. Louis County          MO                       0.0                       0             0.0               0.0\n",
       "1122475  2020-12-13  36053    Madison County          NY                       0.0                       0             0.0               0.0\n",
       "1122476  2020-12-13  21017    Bourbon County          KY                       0.0                       0             0.0               0.0\n",
       "\n",
       "[1063143 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\"Loading Raw Vaccine Data\")\n",
    "#//*** read Raw Vaccine csv\n",
    "county_vax_df = pd.read_pickle(county_vaccine_data_filename)\n",
    "\n",
    "#//*** Filter Columns to get just the Completed Values\n",
    "cols = ['Date','FIPS','Recip_County','Recip_State','Series_Complete_Pop_Pct','Series_Complete_Yes','Administered_Dose1_Pop_Pct','Administered_Dose1_Recip']\n",
    "\n",
    "\n",
    "#//*** remove States not in continental US\n",
    "county_vax_df = county_vax_df[county_vax_df[\"Recip_State\"] != \"AK\" ]\n",
    "county_vax_df = county_vax_df[county_vax_df[\"Recip_State\"] != \"HI\" ]\n",
    "county_vax_df = county_vax_df[county_vax_df[\"Recip_State\"] != \"AS\" ]\n",
    "county_vax_df = county_vax_df[county_vax_df[\"Recip_State\"] != \"GU\" ]\n",
    "county_vax_df = county_vax_df[county_vax_df[\"Recip_State\"] != \"MP\" ]\n",
    "county_vax_df = county_vax_df[county_vax_df[\"Recip_State\"] != \"PR\" ]\n",
    "county_vax_df = county_vax_df[county_vax_df[\"Recip_State\"] != \"VI\" ]\n",
    "county_vax_df = county_vax_df[county_vax_df[\"FIPS\"] != \"UNK\" ]\n",
    "county_vax_df['FIPS'] = county_vax_df['FIPS'].astype(int)\n",
    "county_vax_df['Date'] = county_vax_df['Date'].apply(lambda x: datetime.strptime(x, \"%m/%d/%Y\").date())\n",
    "\n",
    "county_vax_df = county_vax_df[cols]\n",
    "\n",
    "#//*** Cleanup Column Names\n",
    "ren_cols = {\n",
    "'Administered_Dose1_Recip' : 'first_dose_count',\n",
    "'Administered_Dose1_Pop_Pct' : 'first_dose_pct',\n",
    "'Series_Complete_Yes' : 'total_vaccinated_count',\n",
    "'Series_Complete_Pop_Pct' : 'total_vaccinated_percent',\n",
    "}\n",
    "\n",
    "cols = list(county_vax_df.columns)\n",
    "\n",
    "for find,replace in ren_cols.items():\n",
    "    cols = [replace if i==find else i for i in cols]\n",
    "\n",
    "county_vax_df.columns=cols\n",
    "    \n",
    "county_vax_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trash This?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "if True:\n",
    "    \n",
    "    ref_county_df = county_daily_df[county_daily_df['Date'] == county_daily_df['Date'].max()]\n",
    "    thresh_pop = (ref_county_df['Population'].sum()) \n",
    "    print(thresh_pop)\n",
    "    thresh_pop = thresh_pop *.025\n",
    "    print(thresh_pop)\n",
    "    #//*** Associate FIPS with Vaccination Rates\n",
    "    county_tier_dict = {}\n",
    "\n",
    "    working_df = county_vax_df[county_vax_df['Date'] == county_daily_df['Date'].max()]\n",
    "\n",
    "    #//*** Remove counties with 0 percent Vaccinations. They are likely non-reporting counties (like the entire state of texas)\n",
    "    working_df = working_df[working_df['total_vaccinated_percent'] > 0]\n",
    "\n",
    "    print(working_df['total_vaccinated_percent'].max())\n",
    "    print(working_df['total_vaccinated_percent'].min())\n",
    "\n",
    "    small_fips = []\n",
    "    big_fips = []\n",
    "    #//*** Group FIPS by Total vaccinated Percent\n",
    "    #//*** Put Results in Dictionary\n",
    "    for i in range(20):\n",
    "        low_i = i * 5\n",
    "        high_i = low_i+5\n",
    "        loop_df = working_df[ (working_df['total_vaccinated_percent'] >= low_i) & (working_df['total_vaccinated_percent'] < high_i) ]\n",
    "        if len(loop_df) > 0:\n",
    "            #print(loop_df['FIPS'].astype(int).unique())\n",
    "            FIPS_list = list(loop_df['FIPS'].astype(int).unique())\n",
    "            \n",
    "            #//*** IF FIPS < than 2.5% of County Population Save FIPS to rolled into either extreme of the list\n",
    "            if ref_county_df[ref_county_df['FIPS'].isin(FIPS_list)]['Population'].sum() < thresh_pop:\n",
    "                #//*** First Half goes to small_fips\n",
    "                if i < 10:\n",
    "                    small_fips.extend(FIPS_list)\n",
    "                    #print(i,ref_county_df[ref_county_df['FIPS'].isin(FIPS_list)]['Population'].sum(),thresh_pop)\n",
    "                    print(i,len(small_fips),len(FIPS_list))\n",
    "                    continue\n",
    "                #//*** Second Half goes to big_fips\n",
    "                else:\n",
    "                    big_fips.extend(FIPS_list)\n",
    "                    print(\"b\",i,len(big_fips))\n",
    "                    continue\n",
    "            county_tier_dict[low_i] = FIPS_list\n",
    "print(i,len(small_fips),len(county_tier_dict[list(county_tier_dict.keys())[0]]))\n",
    "county_tier_dict[list(county_tier_dict.keys())[0]] = county_tier_dict[list(county_tier_dict.keys())[0]] + small_fips\n",
    "print(len(county_tier_dict[list(county_tier_dict.keys())[0]]))\n",
    "print(i,len(big_fips),len(county_tier_dict[list(county_tier_dict.keys())[-1]]))\n",
    "county_tier_dict[list(county_tier_dict.keys())[-1]] = county_tier_dict[list(county_tier_dict.keys())[-1]] + big_fips \n",
    "len( county_tier_dict[list(county_tier_dict.keys())[-1]])\n",
    "#county_tier_dict\n",
    "\"\"\"\n",
    "print(\"Trash This?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trash this?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "    last_day_df = county_tier_df[county_tier_df['Date'] == county_tier_df['Date'].max()]\n",
    "    last_day_df\n",
    "\n",
    "    us_pop = last_day_df['Population'].sum()\n",
    "    lo_vals = last_day_df[last_day_df['Population'] < us_pop * .025]['tier'].str.replace(\"total_\",\"\").astype(int)\n",
    "\n",
    "\n",
    "    lo_vals = list(\"total_\" + lo_vals[lo_vals < 50].astype(str))\n",
    "    hi_vals = last_day_df[last_day_df['Population'] < us_pop * .025]['tier'].str.replace(\"total_\",\"\").astype(int)\n",
    "    hi_vals = list(\"total_\" + hi_vals[hi_vals > 50].astype(str))\n",
    "    lo_assim = -1\n",
    "    hi_assim = -1\n",
    "    for i in range(len(county_tier_df['tier'].unique())):\n",
    "        if lo_vals[-1] == county_tier_df['tier'].unique()[i]:\n",
    "            lo_assim = county_tier_df['tier'].unique()[i+1]\n",
    "        if hi_vals[0] == county_tier_df['tier'].unique()[i]:\n",
    "            hi_assim = county_tier_df['tier'].unique()[i-1]\n",
    "\n",
    "    #//*** Get FIPS from lo_assim        \n",
    "    lo_FIPS = county_tier_df[county_tier_df['tier'] == lo_assim]['FIPS'].iloc[0]\n",
    "\n",
    "    #//*** Get FIPS from lo_assim        \n",
    "    hi_FIPS = county_tier_df[county_tier_df['tier'] == hi_assim]['FIPS'].iloc[0]\n",
    "\n",
    "    #//*** Merge FIPS with lo_assim FIPS\n",
    "    for tier in lo_vals:\n",
    "        lo_FIPS = lo_FIPS + county_tier_df[county_tier_df['tier'] == tier]['FIPS'].iloc[0]\n",
    "\n",
    "    #//*** Merge FIPS with lo_assim FIPS\n",
    "    for tier in hi_vals:\n",
    "        hi_FIPS = hi_FIPS + county_tier_df[county_tier_df['tier'] == tier]['FIPS'].iloc[0]\n",
    "\n",
    "\n",
    "    for val in lo_vals:\n",
    "            county_tier_df['tier'] = county_tier_df['tier'].str.replace(val,lo_assim)\n",
    "    for val in hi_vals:\n",
    "            county_tier_df['tier'] = county_tier_df['tier'].str.replace(val,hi_assim)\n",
    "\"\"\"\n",
    "print(\"Trash this?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build_tiers_county\"></a> \n",
    "# Build Vaccine Tiers: County Data: county_tier_df #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding Cached County Tier Data. This will take a while.\n",
      "99.9\n",
      "1.4\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#if use_cached_files:\n",
    "#    print(\"Loading Cached County Tier Data\")\n",
    "#    county_tier_df = pd.read_pickle(county_tier_df_filename)\n",
    "if True:\n",
    "    ref_county_df = county_daily_df[county_daily_df['Date'] == county_daily_df['Date'].max()]\n",
    "    thresh_pop = (ref_county_df['Population'].sum()) \n",
    "    thresh_pop = thresh_pop *.025\n",
    "    \n",
    "    print(\"Rebuilding Cached County Tier Data. This will take a while.\")\n",
    "    #print(county_vax_df['Date'].min(),county_vax_df['Date'].max())\n",
    "    #print(county_daily_df['Date'].min(),county_daily_df['Date'].max())\n",
    "    start_date = county_vax_df['Date'].min()\n",
    "    end_date = county_daily_df['Date'].max()\n",
    "\n",
    "    #//*** Associate FIPS with Vaccination Rates\n",
    "    county_tier_dict = {}\n",
    "\n",
    "    working_df = county_vax_df[county_vax_df['Date'] == county_daily_df['Date'].max()]\n",
    "\n",
    "    #//*** Remove counties with 0 percent Vaccinations. They are likely non-reporting counties (like the entire state of texas)\n",
    "    working_df = working_df[working_df['total_vaccinated_percent'] > 0]\n",
    "\n",
    "    print(working_df['total_vaccinated_percent'].max())\n",
    "    print(working_df['total_vaccinated_percent'].min())\n",
    "    small_fips = []\n",
    "    big_fips = []\n",
    "    #//*** Group FIPS by Total vaccinated Percent\n",
    "    #//*** Put Results in Dictionary\n",
    "    for i in range(20):\n",
    "        low_i = i * 5\n",
    "        high_i = low_i+5\n",
    "        loop_df = working_df[ (working_df['total_vaccinated_percent'] >= low_i) & (working_df['total_vaccinated_percent'] < high_i) ]\n",
    "        if len(loop_df) > 0:\n",
    "            FIPS_list = list(loop_df['FIPS'].astype(int).unique())\n",
    "            \n",
    "            #//*** IF FIPS < than 2.5% of County Population Save FIPS to rolled into either extreme of the list\n",
    "            if ref_county_df[ref_county_df['FIPS'].isin(FIPS_list)]['Population'].sum() < thresh_pop:\n",
    "                #//*** First Half goes to small_fips\n",
    "                if i < 10:\n",
    "                    small_fips.extend(FIPS_list)\n",
    "                    continue\n",
    "                #//*** Second Half goes to big_fips\n",
    "                else:\n",
    "                    big_fips.extend(FIPS_list)\n",
    "                    continue\n",
    "            county_tier_dict[low_i] = FIPS_list\n",
    "    \n",
    "    #//*** Add Small FIPS to lowest tier\n",
    "    county_tier_dict[list(county_tier_dict.keys())[0]] = county_tier_dict[list(county_tier_dict.keys())[0]] + small_fips\n",
    "    \n",
    "    #//*** Add Big FIPS to highest tier\n",
    "    county_tier_dict[list(county_tier_dict.keys())[-1]] = county_tier_dict[list(county_tier_dict.keys())[-1]] + big_fips \n",
    "\n",
    "    #//*** Reset Working_df    \n",
    "    #//*** Remove counties with 0 percent Vaccinations. They are likely non-reporting counties (like the entire state of texas)\n",
    "    working_df = county_daily_df[county_daily_df['Date'] >= start_date]\n",
    "    #//*********************************\n",
    "    #//*** Build county_tier_df values\n",
    "    #//*********************************\n",
    "    county_tier_df = pd.DataFrame()\n",
    "\n",
    "    sum_cols_county = ['Population','tot_confirm','tot_deaths']\n",
    "    sum_cols_vax = ['total_vaccinated_count','first_dose_count']\n",
    "    reset_cols = ['New_Confirm','New_Deaths','case_7_day_avg','death_7_day_avg','case_100k_avg','death_100k_avg','case_scaled_100k','death_scaled_100k','total_vaccinated_percent','first_dose_pct']\n",
    "\n",
    "    for tier,FIPS_list in county_tier_dict.items():\n",
    "        print(tier)\n",
    "        date_df = pd.DataFrame()\n",
    "\n",
    "        #//*** Get the dates for these specific FIPS\n",
    "        #//*** Sum the values in the SUM cols. \n",
    "        for group in working_df[working_df['FIPS'].isin(FIPS_list)].groupby('Date'):\n",
    "            #//*** Handles the output of this FIPS/DATE\n",
    "\n",
    "            if group[0] not in county_vax_df['Date'].unique():\n",
    "                continue\n",
    "\n",
    "            #//*** Get the first row, this will hold all the columns and attributes\n",
    "            out_df = group[1].iloc[0].copy()\n",
    "\n",
    "\n",
    "            #//*** Sum appropriate columns for aggregation\n",
    "            for col in sum_cols_county:\n",
    "                out_df[col] = group[1][col].sum()\n",
    "\n",
    "            t_vax_df = county_vax_df[(county_vax_df['FIPS'].isin(FIPS_list))]\n",
    "\n",
    "            t_vax_df = t_vax_df[t_vax_df['Date'] == group[0]]\n",
    "\n",
    "\n",
    "            out_df['total_vaccinated_percent'] = -1\n",
    "            out_df['total_vaccinated_count'] = t_vax_df['total_vaccinated_count'].sum()\n",
    "            out_df['first_dose_pct'] = -1\n",
    "            out_df['first_dose_count'] = t_vax_df['first_dose_count'].sum()\n",
    "\n",
    "\n",
    "            #//*** Set derived Columns to -1. This is a programming fail safe to make sure we get them all\n",
    "            for col in reset_cols:\n",
    "                out_df[col] = -1\n",
    "\n",
    "            #//*** Keep a list of all FIPS values in tier, just in case\n",
    "            out_df['FIPS'] = FIPS_list\n",
    "\n",
    "            out_df['tier'] = f\"total_{tier}\"\n",
    "            #//*** Add this single row based on date\n",
    "            date_df = pd.concat([date_df,pd.DataFrame(out_df).transpose()])\n",
    "\n",
    "        #//*** County/FIPS have been aggregated by date. Build the derived data\n",
    "\n",
    "\n",
    "\n",
    "        date_df['New_Confirm'] = date_df['tot_confirm'].diff(2)\n",
    "        date_df['New_Deaths'] = date_df['tot_deaths'].diff(2)\n",
    "\n",
    "        #//*** Reset Negative Confirmed to 0\n",
    "        date_df.loc[date_df['New_Confirm'] < 0,f'New_Confirm']=0\n",
    "\n",
    "        #//*** Reset Negative Deaths to 0\n",
    "        date_df.loc[date_df['New_Deaths'] < 0,f'New_Deaths']=0\n",
    "\n",
    "        date_df['case_7_day_avg'] = date_df['New_Confirm'].rolling(7).mean()\n",
    "        date_df['death_7_day_avg'] = date_df['New_Deaths'].rolling(7).mean()\n",
    "\n",
    "        date_df['case_100k_avg'] = date_df['case_7_day_avg'] / (date_df['Population'] / 100000 )\n",
    "        date_df['death_100k_avg'] = date_df['death_7_day_avg'] / (date_df['Population'] / 100000 )\n",
    "\n",
    "        #//*** Cap 100k values at 100 & 5 for plotting\n",
    "        date_df['case_scaled_100k'] = date_df['case_100k_avg']\n",
    "        date_df['death_scaled_100k'] = date_df['death_100k_avg']\n",
    "\n",
    "        date_df.loc[date_df[f\"case_scaled_100k\"] > 100,f\"case_scaled_100k\"]=100\n",
    "        date_df.loc[date_df[f\"death_scaled_100k\"] > 5,f\"death_scaled_100k\"]=5\n",
    "\n",
    "        date_df['total_vaccinated_percent'] = ((date_df['total_vaccinated_count'] / date_df['Population']) * 100)\n",
    "        date_df['first_dose_pct'] = ((date_df['first_dose_count'] / date_df['Population']) * 100)\n",
    "\n",
    "        date_df = date_df.dropna()\n",
    "        county_tier_df = pd.concat([county_tier_df,date_df])\n",
    "\n",
    "\n",
    "    #//*** Cap Vaccinations at 100%. Some people must travel out of county to get vaccinated\n",
    "    county_tier_df.loc[county_tier_df[f\"total_vaccinated_percent\"] > 100,f\"total_vaccinated_percent\"]=100\n",
    "    county_tier_df.loc[county_tier_df[f\"first_dose_pct\"] > 100,f\"first_dose_pct\"]=100\n",
    "    county_tier_df.to_pickle(county_tier_df_filename)\n",
    "\n",
    "    county_tier_df\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buiding Whole County Tier\n",
      "total_30\n",
      "total_35\n",
      "total_40\n",
      "total_45\n",
      "total_50\n",
      "total_55\n",
      "total_60\n",
      "total_65\n",
      "total_70\n",
      "total_75\n",
      "Writing Whole_county_tier_df to file....\n",
      "           Date                                               FIPS  Admin2 Province_State         Combined_Key Population tot_confirm tot_deaths New_Confirm New_Deaths  case_7_day_avg  \\\n",
      "15   2020-02-06  [1053, 48401, 48379, 29131, 13057, 18047, 1302...    Bibb        Alabama    Bibb, Alabama, US   16978336           0          0           0          0        0.000000   \n",
      "16   2020-02-07  [1053, 48401, 48379, 29131, 13057, 18047, 1302...    Bibb        Alabama    Bibb, Alabama, US   16978336           0          0           0          0        0.000000   \n",
      "17   2020-02-08  [1053, 48401, 48379, 29131, 13057, 18047, 1302...    Bibb        Alabama    Bibb, Alabama, US   16978336           0          0           0          0        0.000000   \n",
      "18   2020-02-09  [1053, 48401, 48379, 29131, 13057, 18047, 1302...    Bibb        Alabama    Bibb, Alabama, US   16978336           0          0           0          0        0.000000   \n",
      "19   2020-02-10  [1053, 48401, 48379, 29131, 13057, 18047, 1302...    Bibb        Alabama    Bibb, Alabama, US   16978336           0          0           0          0        0.000000   \n",
      "..          ...                                                ...     ...            ...                  ...        ...         ...        ...         ...        ...             ...   \n",
      "662  2021-11-14  [26089, 24027, 49043, 8079, 8113, 6075, 30087,...  Apache        Arizona  Apache, Arizona, US   12164941     1640136      28643        1511         14     2912.428571   \n",
      "663  2021-11-15  [26089, 24027, 49043, 8079, 8113, 6075, 30087,...  Apache        Arizona  Apache, Arizona, US   12164941     1641948      28654        2689         12     2882.285714   \n",
      "664  2021-11-16  [26089, 24027, 49043, 8079, 8113, 6075, 30087,...  Apache        Arizona  Apache, Arizona, US   12164941     1643070      28664        2934         21     2932.142857   \n",
      "665  2021-11-17  [26089, 24027, 49043, 8079, 8113, 6075, 30087,...  Apache        Arizona  Apache, Arizona, US   12164941     1644474      28674        2526         20     2971.142857   \n",
      "666  2021-11-18  [26089, 24027, 49043, 8079, 8113, 6075, 30087,...  Apache        Arizona  Apache, Arizona, US   12164941     1645873      28685        2803         21     3057.000000   \n",
      "\n",
      "     death_7_day_avg case_100k_avg death_100k_avg case_scaled_100k death_scaled_100k      tier  \n",
      "15          0.000000           0.0            0.0              0.0               0.0  total_30  \n",
      "16          0.000000           0.0            0.0              0.0               0.0  total_30  \n",
      "17          0.000000           0.0            0.0              0.0               0.0  total_30  \n",
      "18          0.000000           0.0            0.0              0.0               0.0  total_30  \n",
      "19          0.000000           0.0            0.0              0.0               0.0  total_30  \n",
      "..               ...           ...            ...              ...               ...       ...  \n",
      "662        16.000000     23.941165       0.131526        23.941165          0.131526  total_75  \n",
      "663        16.142857      23.69338         0.1327         23.69338            0.1327  total_75  \n",
      "664        16.428571     24.103223       0.135049        24.103223          0.135049  total_75  \n",
      "665        16.571429     24.423816       0.136223        24.423816          0.136223  total_75  \n",
      "666        18.142857     25.129592       0.149141        25.129592          0.149141  total_75  \n",
      "\n",
      "[6520 rows x 17 columns]\n",
      "2020-01-29 2020-12-21\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#if use_cached_files:\n",
    "#    print(\"Loading Cached Whole County Tier Data\")\n",
    "#    whole_county_tier_df = pd.read_pickle(whole_county_tier_df_filename)\n",
    "if True:\n",
    "    #//*** Set Some Constants\n",
    "    \n",
    "   \n",
    "    #start_summer_date =  datetime(2021, 1, 7).date()\n",
    "    print(\"Buiding Whole County Tier\")\n",
    "    whole_county_tier_df = pd.DataFrame()\n",
    "\n",
    "    #working_df = county_daily_df[county_daily_df['Date'] < start_summer_date]\n",
    "    reset_cols = ['New_Confirm','New_Deaths','case_7_day_avg','death_7_day_avg','case_100k_avg','death_100k_avg','case_scaled_100k','death_scaled_100k']\n",
    "    sum_cols_county = ['Population','tot_confirm','tot_deaths']\n",
    "\n",
    "    #//*** Build list of tiers and FIPS from final Day or County_tier_df\n",
    "    for row in county_tier_df[county_tier_df['Date'] == county_tier_df['Date'].max()][['tier','FIPS']].iterrows():\n",
    "        tier = row[1]['tier']\n",
    "        print(tier)\n",
    "        FIPS_list = row[1]['FIPS']\n",
    "        date_df = pd.DataFrame()\n",
    "        for group in county_daily_df[county_daily_df['FIPS'].isin(FIPS_list)].groupby('Date'):\n",
    "\n",
    "            #//*** Get the first row, this will hold all the columns and attributes\n",
    "            out_df = group[1].iloc[0].copy()\n",
    "\n",
    "            #//*** Sum appropriate columns for aggregation\n",
    "            for col in sum_cols_county:\n",
    "                out_df[col] = group[1][col].sum()\n",
    "\n",
    "            #//*** Set derived Columns to -1. This is a programming fail safe to make sure we get them all\n",
    "            for col in reset_cols:\n",
    "                out_df[col] = -1\n",
    "\n",
    "            #//*** Keep a list of all FIPS values in tier, just in case\n",
    "            out_df['FIPS'] = FIPS_list\n",
    "\n",
    "            out_df['tier'] = f\"{tier}\"\n",
    "\n",
    "            date_df = pd.concat([date_df,pd.DataFrame(out_df).transpose()])\n",
    "\n",
    "\n",
    "        date_df['New_Confirm'] = date_df['tot_confirm'].diff(2)\n",
    "        date_df['New_Deaths'] = date_df['tot_deaths'].diff(2)\n",
    "\n",
    "        #//*** Reset Negative Confirmed to 0\n",
    "        date_df.loc[date_df['New_Confirm'] < 0,f'New_Confirm']=0\n",
    "\n",
    "        #//*** Reset Negative Deaths to 0\n",
    "        date_df.loc[date_df['New_Deaths'] < 0,f'New_Deaths']=0\n",
    "\n",
    "        date_df['case_7_day_avg'] = date_df['New_Confirm'].rolling(7).mean()\n",
    "        date_df['death_7_day_avg'] = date_df['New_Deaths'].rolling(7).mean()\n",
    "\n",
    "        date_df['case_100k_avg'] = date_df['case_7_day_avg'] / (date_df['Population'] / 100000 )\n",
    "        date_df['death_100k_avg'] = date_df['death_7_day_avg'] / (date_df['Population'] / 100000 )\n",
    "\n",
    "        #//*** Cap 100k values at 100 & 5 for plotting\n",
    "        date_df['case_scaled_100k'] = date_df['case_100k_avg']\n",
    "        date_df['death_scaled_100k'] = date_df['death_100k_avg']\n",
    "\n",
    "        date_df.loc[date_df[f\"case_scaled_100k\"] > 100,f\"case_scaled_100k\"]=100\n",
    "        date_df.loc[date_df[f\"death_scaled_100k\"] > 5,f\"death_scaled_100k\"]=5\n",
    "\n",
    "        date_df = date_df.dropna()\n",
    "       \n",
    "        whole_county_tier_df = pd.concat([whole_county_tier_df,date_df])\n",
    "    print(\"Writing Whole_county_tier_df to file....\")\n",
    "    whole_county_tier_df.to_pickle(whole_county_tier_df_filename)\n",
    "    print(whole_county_tier_df)\n",
    "    print(county_daily_df['Date'].min(),county_tier_df['Date'].min())\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['total_30', 'total_35', 'total_40', 'total_45', 'total_50',\n",
       "       'total_55', 'total_60', 'total_65', 'total_70', 'total_75'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_county_tier_df['tier'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ Graph Tiers: county_tier_df](#graph_tiers_county)\n",
    "<a id=\"graph_tiers_county\"></a> \n",
    "# Graph Tiers: county_tier_df #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Statewide Overview Summary Dictionary #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statewide Vax Tier Graphs #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Statewide Overview Summer Summary Dictionary #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer Statewide Vax Tier Graphs #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ Analyze State Hospitalization Tiers: state_hosp_tiers_df](#analyze_state_hosp_tiers_df)\n",
    "<a id=\"analyze_state_hosp_tiers_df\"></a> \n",
    "# Analyze State Hospitalization Tiers: state_hosp_tiers_df #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ Blog Graphs ](#build_blog_graphs)\n",
    "<a id=\"build_blog_graphs\"></a> \n",
    "# Blog: Graphs #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Blog Post: COVID Vaccine Comparison Page ##\n",
    "- [ Blog Graphs ](#build_blog_graphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
